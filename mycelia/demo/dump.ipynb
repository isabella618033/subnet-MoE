{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317eaab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.9.1 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isabella/crucible/subnet-MoE/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mycelia.shared.config import MinerConfig, ValidatorConfig\n",
    "from mycelia.shared.cycle import get_validator_miner_assignment\n",
    "from mycelia.miner.train import * \n",
    "import bittensor as bt \n",
    "\n",
    "subtensor = bt.subtensor('test')\n",
    "\n",
    "path = \"/home/isabella/crucible/subnet-MoE/checkpoints/miner/miner/hk1/foundation/config.yaml\"\n",
    "config = MinerConfig.from_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f6e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-12-12 23:50:46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrank 2: No checkpoints found in /home/isabella/crucible/subnet-MoE/checkpoints/miner/miner/hk1/foundation\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.checkpoint\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:50:46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mload_expert_group_assignment - folder\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m \u001b[36mpositional_args\u001b[0m=\u001b[35m(PosixPath('/home/isabella/crucible/subnet-MoE/expert_groups/exp_dummy'),)\u001b[0m\n",
      "\u001b[2m2025-12-12 23:50:46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mload_expert_group_assignment - folder\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m \u001b[36mpositional_args\u001b[0m=\u001b[35m(PosixPath('/home/isabella/crucible/subnet-MoE/expert_groups/exp_math'),)\u001b[0m\n",
      "\u001b[2m2025-12-12 23:50:49\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFetching model from chain   \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.chain\u001b[0m]\u001b[0m \u001b[36mshould_download\u001b[0m=\u001b[35mFalse\u001b[0m\n",
      "\u001b[2m2025-12-12 23:50:49\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGet base model for checkpoint\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.model\u001b[0m]\u001b[0m \u001b[36mgroup_ids\u001b[0m=\u001b[35m[1]\u001b[0m \u001b[36mpartial\u001b[0m=\u001b[35mTrue\u001b[0m\n",
      "\u001b[2m2025-12-12 23:50:49\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoading in standard mode for miner\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.modeling.mycelia\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of the required library is not installed. Falling back to torch implementation. To install follow https://github.com/fla-org/flash-linear-attention#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-12-12 23:51:48\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoad base model for checkpoint\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.model\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:51:48\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrank 2: No checkpoints found in /home/isabella/crucible/subnet-MoE/checkpoints/miner/miner/hk1/foundation\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.checkpoint\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:51:48\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrank 2: No checkpoints found in /home/isabella/crucible/subnet-MoE/checkpoints/miner/miner/hk1/foundation\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.checkpoint\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:51:48\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1msecondary checkpoint not found\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.checkpoint\u001b[0m]\u001b[0m \u001b[36msecondary_ckpt_path\u001b[0m=\u001b[35mPosixPath('/home/isabella/crucible/subnet-MoE/checkpoints/miner/miner/hk1/foundation')\u001b[0m\n",
      "\u001b[2m2025-12-12 23:51:48\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mTried to resume from checkpoint, but no checkpoint found.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.model\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:53:30\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1m rank 2 optimizer           \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.miner.train\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:53:30\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1m rank 2 scheduler           \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.miner.train\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-12-12 23:53:32\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrank 2 setup_training: success!\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.miner.train\u001b[0m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rank = 2\n",
    "# === set up chain worker ===\n",
    "wallet, subtensor = setup_chain_worker(config)\n",
    "\n",
    "# === mis ===\n",
    "device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = get_base_tokenizer(config)\n",
    "\n",
    "# === set up training ===\n",
    "(\n",
    "    model,\n",
    "    global_model,\n",
    "    inner_optimizer,\n",
    "    outer_optimizer,\n",
    "    inner_scaler,\n",
    "    outer_scaler,\n",
    "    scheduler,\n",
    "    # start_step,\n",
    "    expert_manager,\n",
    "    train_dataloader,\n",
    "    current_model_meta,\n",
    ") = setup_training(config, rank, device, tokenizer, subtensor, wallet, current_model_meta=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bfdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0274d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): CustomQwen3NextModel(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (4-6): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (8-10): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (12-14): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (15): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (16-18): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (19): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (20-22): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (23): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (24-26): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (27): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (28-30): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (31): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (32-34): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (35): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (36-38): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (39): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (40-42): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (43): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (44-46): 3 x DecoderLayer(\n",
       "        (linear_attn): Qwen3NextGatedDeltaNet(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n",
       "          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n",
       "          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n",
       "          (norm): Qwen3NextRMSNormGated()\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (47): DecoderLayer(\n",
       "        (self_attn): Qwen3NextAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): SparseMoeBlock(\n",
       "          (gate): TopKRouter(\n",
       "            (weight): Linear(in_features=2048, out_features=11, bias=False)\n",
       "          )\n",
       "          (experts): ModuleDict(\n",
       "            (0): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (1): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (2): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (3): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (4): Qwen3NextMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (shared_expert): Qwen3NextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
       "        )\n",
       "        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3NextRotaryEmbedding()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f45cd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-12-13 00:18:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mbatch device keys           \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.miner.train\u001b[0m]\u001b[0m \u001b[36mpositional_args\u001b[0m=\u001b[35m(dict_keys(['input_ids', 'attention_mask', 'labels']),)\u001b[0m\n",
      "\u001b[2m2025-12-13 00:18:10\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mmodel output                \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.miner.train\u001b[0m]\u001b[0m \u001b[36mpositional_args\u001b[0m=\u001b[35m(MoeModelOutputWithPast(last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:2',\n",
      "       grad_fn=<CompiledFunctionBackward>), past_key_values=None, hidden_states=None, attentions=None, router_logits=None),)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter_data)\n",
    "\n",
    "model.train()\n",
    "\n",
    "batch_device = {}\n",
    "for key in batch.keys():\n",
    "    batch_device[key] = batch[key].to(device)\n",
    "\n",
    "with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "    logger.info(\"batch device keys\", batch_device.keys())\n",
    "    outputs = model(**batch_device)\n",
    "\n",
    "    logger.info(\"model output\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
