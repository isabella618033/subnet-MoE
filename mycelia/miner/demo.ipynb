{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mycelia.shared.model import load_base_model\n",
    "from mycelia.shared.modeling.modeling_mycelia import get_base_tokenizer\n",
    "from mycelia.config import Config\n",
    "from mycelia.shared.datasets import get_dataloader\n",
    "from mycelia.shared.logging import configure_logging\n",
    "from mycelia.shared.helper import get_nested_attr\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "config = Config()\n",
    "config.model.device = \"cuda:2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e00f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config.moe.partial_moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5c5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-10-07 02:21:20\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrank 0: Checkpoint path checkpoints/centralised not found, starting from scratch\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.checkpoint\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating DeepseekAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.\n",
      "\u001b[2m2025-10-07 02:22:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mem init                     \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-10-07 02:22:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mem get expert layer         \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-10-07 02:22:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mem get expert layer 1       \u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-10-07 02:22:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDetected expert layerss: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27]\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m\n",
      "\u001b[2m2025-10-07 02:22:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mseed 0 Expert group assignment for layer 27: {0: [1, 4, 5, 3], 1: [7, 2, 6, 0]}\u001b[0m [\u001b[0m\u001b[1m\u001b[34mmycelia.shared.expert_manager\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "model, em = load_base_model(config = config, rank = 0)\n",
    "# tokenizer = get_base_tokenizer(config)\n",
    "# dataloader = get_dataloader(\n",
    "#     config,\n",
    "#     rank= config.data.rank,\n",
    "#     world_size= config.data.world_size,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a091ffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.79590144"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model: nn.Module, trainable_only: bool = False):\n",
    "    \"\"\"\n",
    "    Count parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        trainable_only (bool): If True, count only parameters that require gradients.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of parameters.\n",
    "    \"\"\"\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "count_parameters(model) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6535b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:48<00:00, 15.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    OlmoForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    PretrainedConfig\n",
    ")\n",
    "org_model = AutoModelForCausalLM.from_pretrained(config.model.model_path, trust_remote_code=True)\n",
    "output = org_model(**batch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e73e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(data_iter)  \n",
    "\n",
    "batch_device = {}\n",
    "for key in batch.keys():\n",
    "    batch_device[key] = batch[key].to(model.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
